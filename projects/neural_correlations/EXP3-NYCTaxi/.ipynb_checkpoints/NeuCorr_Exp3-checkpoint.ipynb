{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 3-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from nupic.encoders import ScalarEncoder\n",
    "from nupic.bindings.algorithms import TemporalMemory as TM\n",
    "from nupic.bindings.algorithms import SpatialPooler as SP\n",
    "from htmresearch.support.neural_correlations_utils import *\n",
    "\n",
    "uintType = \"uint32\"\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSize = 109\n",
    "maxItems = 17520\n",
    "totalTS = maxItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('nyc_taxi.csv', skiprows=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm = TM(columnDimensions = (2048,),\n",
    "        cellsPerColumn=8, # We changed here the number of cells per col, initially they were 32\n",
    "        initialPermanence=0.21,\n",
    "        connectedPermanence=0.3,\n",
    "        minThreshold=15,\n",
    "        maxNewSynapseCount=40,\n",
    "        permanenceIncrement=0.1,\n",
    "        permanenceDecrement=0.1,\n",
    "        activationThreshold=15,\n",
    "        predictedSegmentDecrement=0.01\n",
    "       )\n",
    "\n",
    "sparsity = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)\n",
    "\n",
    "sp = SP(inputDimensions=(inputSize,),\n",
    "        columnDimensions=(2048,),\n",
    "        potentialRadius = int(0.5*inputSize),\n",
    "        numActiveColumnsPerInhArea = sparseCols,\n",
    "        globalInhibition = True,\n",
    "        synPermActiveInc = 0.0001,\n",
    "        synPermInactiveDec = 0.0005,\n",
    "        synPermConnected = 0.5,\n",
    "        maxBoost = 1.0,\n",
    "        spVerbosity = 1\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "3000 items processed\n",
      "3500 items processed\n",
      "4000 items processed\n",
      "4500 items processed\n",
      "5000 items processed\n",
      "5500 items processed\n",
      "6000 items processed\n",
      "6500 items processed\n",
      "7000 items processed\n",
      "7500 items processed\n",
      "8000 items processed\n",
      "8500 items processed\n",
      "9000 items processed\n",
      "9500 items processed\n",
      "10000 items processed\n",
      "10500 items processed\n",
      "11000 items processed\n",
      "11500 items processed\n",
      "12000 items processed\n",
      "12500 items processed\n",
      "13000 items processed\n",
      "13500 items processed\n",
      "14000 items processed\n",
      "14500 items processed\n",
      "15000 items processed\n",
      "15500 items processed\n",
      "16000 items processed\n",
      "16500 items processed\n",
      "17000 items processed\n",
      "17500 items processed\n",
      "*** All items encoded! ***\n"
     ]
    }
   ],
   "source": [
    "rawValues = []\n",
    "remainingRows = maxItems\n",
    "numTrainingItems = 15000\n",
    "trainSet = []\n",
    "nonTrainSet = []\n",
    "\n",
    "se = ScalarEncoder(n=109, w=29, minval=0, maxval=40000, clipInput=True)\n",
    "s = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if s > 0 and s % 500 == 0:\n",
    "        print str(s) + \" items processed\"\n",
    "        \n",
    "    rawValues.append(row['passenger_count'])\n",
    "    \n",
    "    if s < numTrainingItems:\n",
    "        trainSet.append(se.encode(row['passenger_count']))\n",
    "    else:\n",
    "        nonTrainSet.append(se.encode(row['passenger_count']))\n",
    "        \n",
    "    remainingRows -= 1\n",
    "    s += 1\n",
    "    if remainingRows == 0: \n",
    "        break\n",
    "print \"*** All items encoded! ***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Spatial Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 0\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "*** All items processed! ***\n"
     ]
    }
   ],
   "source": [
    "allSequences = []\n",
    "outputColumns = np.zeros(sp.getNumColumns(), dtype=\"uint32\")\n",
    "columnUsage = np.zeros(sp.getNumColumns(), dtype=\"uint32\")\n",
    "\n",
    "# Set epochs for spatial-pooling:\n",
    "spEpochs = 4\n",
    "\n",
    "for epoch in range(spEpochs):\n",
    "    print \"Training epoch: \" + str(epoch)\n",
    "    \n",
    "    #randomize records in training set\n",
    "    randomIndex = np.random.permutation(np.arange(numTrainingItems))\n",
    "    \n",
    "    for i in range(numTrainingItems):\n",
    "        sp.compute(trainSet[randomIndex[i]], True, outputColumns)\n",
    "        # Populate array for Yuwei plot:\n",
    "        for col in outputColumns.nonzero():\n",
    "            columnUsage[col] += 1                        \n",
    "        if epoch == (spEpochs - 1):\n",
    "            allSequences.append(outputColumns.nonzero()) \n",
    "\n",
    "for i in range(maxItems - numTrainingItems):\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        print str(i) + \" items processed\"    \n",
    "    sp.compute(nonTrainSet[i], False, outputColumns)\n",
    "    allSequences.append(outputColumns.nonzero())\n",
    "    # Populate array for Yuwei plot:\n",
    "    for col in outputColumns.nonzero():\n",
    "        columnUsage[col] += 1                \n",
    "\n",
    "print \"*** All items processed! ***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "plt.hist(columnUsage, bins)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage_SP\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Temporal Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 items processed\n",
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "3000 items processed\n",
      "3500 items processed\n",
      "4000 items processed\n",
      "4500 items processed\n",
      "5000 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "5500 items processed\n",
      "6000 items processed\n",
      "6500 items processed\n",
      "7000 items processed\n",
      "7500 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "8000 items processed\n",
      "8500 items processed\n",
      "9000 items processed\n",
      "9500 items processed\n",
      "10000 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "10500 items processed\n",
      "11000 items processed\n",
      "11500 items processed\n",
      "12000 items processed\n",
      "12500 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "13000 items processed\n",
      "13500 items processed\n",
      "14000 items processed\n",
      "14500 items processed\n",
      "15000 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "15500 items processed\n",
      "16000 items processed\n",
      "16500 items processed\n",
      "17000 items processed\n",
      "17500 items processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "***All items processed!***\n"
     ]
    }
   ],
   "source": [
    "spikeTrains = np.zeros((tm.numberOfCells(), totalTS), dtype = \"uint32\")\n",
    "columnUsage = np.zeros(tm.numberOfColumns(), dtype=\"uint32\")\n",
    "ts = 0\n",
    "\n",
    "entropyX = []\n",
    "entropyY = []\n",
    "\n",
    "negPCCX_cells = []\n",
    "negPCCY_cells = []\n",
    "\n",
    "negPCCX_cols = []\n",
    "negPCCY_cols = []\n",
    "\n",
    "# Randomly generate the indices of the columns to keep track during simulation time\n",
    "colIndices = np.random.permutation(tm.numberOfColumns())[0:4] # keep track of 4 columns\n",
    "\n",
    "for s in range(maxItems):\n",
    "    if s % 500 == 0:\n",
    "        print str(s) + \" items processed\"\n",
    "        \n",
    "    tm.compute(allSequences[s][0].tolist(), learn=True)\n",
    "    for cell in tm.getActiveCells():\n",
    "        spikeTrains[cell, ts] = 1            \n",
    "    # Obtain active columns:\n",
    "    activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells()]\n",
    "    currentColumns = [1 if i in activeColumnsIndices else 0 for i in range(tm.numberOfColumns())]\n",
    "    for col in np.nonzero(currentColumns)[0]:\n",
    "        columnUsage[col] += 1                \n",
    "    \n",
    "    if s > 0 and s % 2500 == 0:\n",
    "        print \"++ Analyzing correlations (cells at random) ++\"                \n",
    "        subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), ts)\n",
    "        (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "        negPCCX_cells.append(s)\n",
    "        negPCCY_cells.append(numNegPCC)                \n",
    "        print \"++ Generating histogram ++\"\n",
    "        bins = 300\n",
    "        plt.hist(corrMatrix.ravel(), bins, alpha=0.5)                \n",
    "        # Set range for plot appropriately!\n",
    "        plt.xlim(-0.05,0.1)\n",
    "        plt.xlabel(\"PCC\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.savefig(\"cellsHist\" + str(s))\n",
    "        plt.close()\n",
    "        # Compute entropy\n",
    "        print \"++ Computing entropy ++\"\n",
    "        entropyX.append(s)\n",
    "        entropyY.append(computeEntropy(subSpikeTrains))  \n",
    "        \n",
    "        print \"++ Analyzing correlations (whole columns) ++\"                \n",
    "        subSpikeTrains = subSampleWholeColumn(spikeTrains, colIndices, tm.getCellsPerColumn(), ts)\n",
    "        (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "        negPCCX_cols.append(s)\n",
    "        negPCCY_cols.append(numNegPCC)                \n",
    "        print \"++ Generating histogram ++\"\n",
    "        bins = 100\n",
    "        plt.hist(corrMatrix.ravel(), bins, alpha=0.5)\n",
    "        plt.xlabel(\"PCC\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.savefig(\"colsHist_\" + str(s))\n",
    "        plt.close() \n",
    "        print \"++ Generating heatmap ++\"\n",
    "        plt.imshow(corrMatrix, cmap='spectral', interpolation='nearest')\n",
    "        cb = plt.colorbar()\n",
    "        cb.set_label('PCC')\n",
    "        plt.savefig(\"colsHeatMap_\" + str(s))\n",
    "        plt.close() \n",
    "        \n",
    "    ts += 1\n",
    "                \n",
    "print \"***All items processed!***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot trace of negative PCCs\n",
    "plt.plot(negPCCX_cells, negPCCY_cells)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cells\")\n",
    "plt.close()\n",
    "\n",
    "plt.plot(negPCCX_cols, negPCCY_cols)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cols\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print computeEntropy()\n",
    "plt.plot(entropyX, entropyY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.savefig(\"entropyTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "plt.hist(columnUsage, bins)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage_TM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Analysis of Spike Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352\n",
      "Accuracy: 0\n",
      "Active cols: [  13   34   60  166  172  191  204  206  228  251  261  279  377  395  478\n",
      "  480  535  572  725  838  883 1061 1186 1189 1217 1348 1364 1384 1460 1562\n",
      " 1588 1667 1670 1680 1686 1782 1812 1878 1912 1963]\n",
      "Predicted cols: [  13   34   60  109  132  147  149  166  172  191  197  200  204  206  219\n",
      "  228  251  258  261  279  315  332  377  395  405  427  478  480  534  535\n",
      "  572  663  725  754  772  801  838  876  883  929  965 1038 1061 1093 1100\n",
      " 1186 1187 1189 1202 1217 1238 1257 1263 1266 1301 1348 1364 1384 1429 1450\n",
      " 1460 1461 1552 1560 1562 1575 1588 1639 1646 1667 1670 1680 1699 1720 1738\n",
      " 1742 1764 1770 1782 1791 1812 1856 1878 1882 1912 1916 1963 2020 2029 2037\n",
      " 2045 2046]\n",
      "\n",
      "Accuracy: 0.0\n",
      "Active cols: [  83  174  185  287  368  453  517  540  697  721  742  762  799  805  893\n",
      "  925  931  949 1041 1050 1069 1085 1132 1183 1210 1236 1259 1280 1357 1395\n",
      " 1426 1481 1618 1717 1795 1837 1845 1863 1903 2002]\n",
      "Predicted cols: [  13   34   60   65  149  191  200  206  219  228  235  251  258  261  315\n",
      "  405  478  572  661  725  754  801  838  965 1100 1186 1187 1217 1238 1257\n",
      " 1263 1266 1348 1384 1429 1460 1461 1560 1562 1575 1588 1680 1720 1764 1791\n",
      " 2020 2037]\n",
      "\n",
      "Accuracy: 0.0\n",
      "Active cols: [   6  169  185  306  343  368  400  440  453  540  670  724  742  764  797\n",
      "  877  894  954 1034 1132 1162 1180 1226 1357 1449 1512 1546 1548 1570 1583\n",
      " 1618 1722 1733 1746 1795 1810 1818 1886 1931 2009]\n",
      "Predicted cols: [  52   83  122  125  166  174  221  228  261  427  491  634  725  752  799\n",
      "  801  805  838  848  861  893  926  928  987 1041 1058 1134 1160 1236 1238\n",
      " 1257 1280 1348 1384 1388 1402 1429 1454 1541 1548 1562 1575 1668 1795 1837\n",
      " 1903]\n",
      "\n",
      "Accuracy: 0.282608695652\n",
      "Active cols: [  13   34   60  149  166  172  191  197  200  206  228  251  261  290  315\n",
      "  405  478  725  801  838 1093 1186 1217 1238 1257 1263 1266 1348 1384 1429\n",
      " 1460 1560 1562 1575 1588 1680 1720 1791 1878 2020]\n",
      "Predicted cols: [  13   34   60   77  116  149  166  191  197  200  206  228  235  251  258\n",
      "  261  315  332  405  478  534  572  723  725  754  772  801  838  883  965\n",
      " 1093 1096 1100 1186 1187 1189 1202 1217 1238 1257 1263 1266 1348 1384 1429\n",
      " 1450 1460 1560 1562 1575 1588 1591 1667 1680 1699 1720 1742 1752 1764 1791\n",
      " 1820 1878 1916 1970 2020 2037 2045]\n",
      "\n",
      "Accuracy: 0.0\n",
      "Active cols: [  15   46   89  474  510  539  540  587  612  701  780  877  891  903 1011\n",
      " 1048 1059 1148 1154 1292 1312 1434 1457 1498 1558 1570 1571 1579 1604 1739\n",
      " 1814 1842 1867 1931 1965 1978 1995 2024 2027 2042]\n",
      "Predicted cols: [  15   60   91   95  166  172  191  200  206  228  261  339  446  478  565\n",
      "  572  610  612  653  725  806  818  837  838  888  946  965 1022 1077 1186\n",
      " 1217 1221 1238 1257 1266 1321 1333 1348 1384 1408 1456 1460 1560 1562 1574\n",
      " 1579 1592 1630 1661 1680 1720 1726 1764 1800 1814 1823 1867 1906 1966 1982\n",
      " 2020]\n",
      "\n",
      "Accuracy: 0.0655737704918\n",
      "Active cols: [  77  107  227  235  329  358  360  392  397  399  426  598  653  690  726\n",
      "  753  776  791  951 1108 1161 1177 1238 1400 1409 1411 1466 1501 1574 1591\n",
      " 1599 1600 1700 1757 1764 1776 1802 1821 1970 2037]\n",
      "Predicted cols: [  13   30   34   60  107  149  166  172  178  191  200  206  228  235  249\n",
      "  251  258  261  296  315  360  377  380  392  393  397  399  405  478  533\n",
      "  549  598  630  653  661  696  725  726  744  776  791  801  831  838  875\n",
      "  951  963  965  986 1029 1053 1093 1100 1108 1161 1186 1189 1217 1238 1257\n",
      " 1262 1263 1266 1270 1348 1384 1429 1460 1480 1540 1554 1560 1562 1574 1575\n",
      " 1600 1667 1670 1680 1720 1726 1764 1776 1791 1821 1828 1843 1853 2020 2037]\n",
      "\n",
      "Accuracy: 0.422222222222\n",
      "Active cols: [  13   34   60  149  166  191  200  206  228  251  261  315  405  478  534\n",
      "  725  801  838  965 1093 1100 1186 1217 1238 1257 1263 1266 1348 1384 1429\n",
      " 1460 1560 1562 1575 1588 1680 1720 1764 1791 2020]\n",
      "Predicted cols: []\n",
      "\n",
      "Accuracy: 0\n",
      "Active cols: [   0   34   60  166  191  200  206  228  235  261  315  332  405  478  534\n",
      "  572  772  801  838  965 1093 1100 1187 1238 1257 1266 1348 1384 1429 1450\n",
      " 1460 1560 1562 1680 1720 1764 1791 1970 2045 2046]\n",
      "Predicted cols: [   0    7   13   34   60   77  107  116  149  151  166  172  191  197  200\n",
      "  206  228  235  251  258  261  272  315  329  332  365  377  405  478  480\n",
      "  534  542  572  577  598  618  653  723  725  754  772  773  776  801  838\n",
      "  883  965 1023 1088 1093 1096 1100 1177 1186 1187 1189 1202 1214 1217 1229\n",
      " 1238 1257 1263 1266 1315 1348 1364 1384 1400 1429 1450 1460 1501 1560 1562\n",
      " 1575 1588 1591 1667 1680 1699 1720 1742 1752 1764 1791 1820 1878 1881 1916\n",
      " 1970 2020 2037 2045 2046]\n",
      "\n",
      "Accuracy: 0.0421052631579\n",
      "Active cols: [  96  163  185  200  252  363  440  468  540  670  686  767  797  802  833\n",
      "  891 1088 1176 1236 1272 1319 1342 1372 1395 1429 1441 1449 1456 1463 1483\n",
      " 1536 1548 1612 1618 1657 1681 1720 1818 1845 1865]\n",
      "Predicted cols: [  32   40   60  122  125  166  200  206  228  261  287  315  544  809  838\n",
      "  893  928 1058 1065 1085 1160 1238 1257 1266 1280 1348 1357 1384 1429 1460\n",
      " 1548 1876 2012]\n",
      "\n",
      "Accuracy: 0.151515151515\n",
      "Active cols: [  78  109  132  148  166  172  191  200  219  261  377  427  480  499  606\n",
      "  852  883 1070 1087 1124 1189 1348 1364 1365 1384 1388 1389 1403 1646 1656\n",
      " 1670 1738 1770 1782 1812 1871 1882 1912 1959 2029]\n",
      "Predicted cols: [   6   13   34   48   60   65  109  125  131  132  147  166  172  191  197\n",
      "  200  204  206  219  228  251  261  279  296  315  377  384  395  405  427\n",
      "  478  480  535  540  572  663  684  725  801  838  845  876  883  929  936\n",
      "  965 1019 1038 1061 1071 1093 1186 1189 1217 1238 1257 1263 1266 1294 1301\n",
      " 1348 1364 1384 1429 1460 1461 1552 1560 1562 1574 1575 1588 1646 1656 1667\n",
      " 1670 1680 1720 1738 1744 1770 1782 1791 1812 1856 1870 1882 1912 2020 2029]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleAccuracyTest(\"periodic\", tm, allSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), totalTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 cells processed\n",
      "500 cells processed\n",
      "750 cells processed\n",
      "**All cells processed**\n"
     ]
    }
   ],
   "source": [
    "isi = computeISI(subSpikeTrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bins = np.linspace(np.min(isi), np.max(isi), 50)\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "# plt.xlim(0,4000)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part V. Save TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saveTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a4e3365d1923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaveTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallSequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'saveTM' is not defined"
     ]
    }
   ],
   "source": [
    "saveTM(allSequences, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load the TM back from the file do:\n",
    "with open('tm.nta', 'rb') as f:\n",
    "    proto2 = TemporalMemoryProto_capnp.TemporalMemoryProto.read(f, traversal_limit_in_words=2**61)\n",
    "tm = TM.read(proto2)\n",
    "# to load sequences from text file do:\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    allSequences = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI. Analysis of Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlapMatrix = inputAnalysis(allSequences, \"periodic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show heatmap of overlap matrix\n",
    "plt.imshow(overlapMatrix, cmap='spectral', interpolation='nearest')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Overlap Score')\n",
    "plt.savefig(\"overlapScore_heatmap\")\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "# generate histogram\n",
    "bins = 60\n",
    "(n, bins, patches) = plt.hist(overlapMatrix.ravel(), bins, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist\")\n",
    "\n",
    "plt.xlim(0.5,1)\n",
    "plt.ylim(0,1000000)\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist_ZOOM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
