{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 1-Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nupic.bindings.algorithms import TemporalMemory as TM\n",
    "from htmresearch.support.neural_correlations_utils import *\n",
    "    \n",
    "uintType = \"uint32\"\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbolsPerSequence = 10\n",
    "numSequences = 1000\n",
    "epochs = 5\n",
    "totalTS = epochs * numSequences * symbolsPerSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm = TM(columnDimensions = (2048,),\n",
    "    cellsPerColumn=8,\n",
    "    initialPermanence=0.21,\n",
    "    connectedPermanence=0.3,\n",
    "    minThreshold=15,\n",
    "    maxNewSynapseCount=40,\n",
    "    permanenceIncrement=0.1,\n",
    "    permanenceDecrement=0.1,\n",
    "    activationThreshold=15,\n",
    "    predictedSegmentDecrement=0.01,\n",
    "    )\n",
    "\n",
    "sparsity = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed sequences to the TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 sequences processed\n",
      "100 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "150 sequences processed\n",
      "200 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "250 sequences processed\n",
      "300 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "350 sequences processed\n",
      "400 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "450 sequences processed\n",
      "500 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "550 sequences processed\n",
      "600 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "650 sequences processed\n",
      "700 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "750 sequences processed\n",
      "800 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "850 sequences processed\n",
      "900 sequences processed\n",
      "++ Analyzing correlations (cells at random) ++\n",
      "++ Generating histogram ++\n",
      "++ Computing entropy ++\n",
      "++ Analyzing correlations (whole columns) ++\n",
      "++ Generating histogram ++\n",
      "++ Generating heatmap ++\n",
      "950 sequences processed\n",
      "***All sequences processed!***\n"
     ]
    }
   ],
   "source": [
    "allSequences = []\n",
    "spikeTrains = np.zeros((tm.numberOfCells(), totalTS), dtype = \"uint32\")\n",
    "columnUsage = np.zeros(tm.numberOfColumns(), dtype=\"uint32\")\n",
    "ts = 0\n",
    "\n",
    "entropyX = []\n",
    "entropyY = []\n",
    "\n",
    "negPCCX_cells = []\n",
    "negPCCY_cells = []\n",
    "\n",
    "negPCCX_cols = []\n",
    "negPCCY_cols = []\n",
    "\n",
    "# Randomly generate the indices of the columns to keep track during simulation time\n",
    "colIndicesSmall = np.random.permutation(tm.numberOfColumns())[0:4] # keep track of 4 columns\n",
    "colIndicesLarge = np.random.permutation(tm.numberOfColumns())[0:125] # keep track of 4 columns\n",
    "\n",
    "for s in range(numSequences):\n",
    "    sequence = generateRandomSequence(symbolsPerSequence, tm.numberOfColumns(), sparsity)\n",
    "    if s > 0 and s % 50 == 0:\n",
    "        print str(s) + \" sequences processed\"\n",
    "        \n",
    "    for t in range(epochs):\n",
    "        for symbol in range(symbolsPerSequence):\n",
    "            tm.compute(sequence[symbol], learn=True)\n",
    "            for cell in tm.getActiveCells():\n",
    "                spikeTrains[cell, ts] = 1            \n",
    "            \n",
    "            # Obtain active columns:\n",
    "            activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells()]\n",
    "            currentColumns = [1 if i in activeColumnsIndices else 0 for i in range(tm.numberOfColumns())]\n",
    "            for col in np.nonzero(currentColumns)[0]:\n",
    "                columnUsage[col] += 1            \n",
    "            \n",
    "            if ts > 0 and ts % int(totalTS * 0.1) == 0:\n",
    "                print \"++ Analyzing correlations (cells at random) ++\"                \n",
    "                subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), ts)\n",
    "                (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "                negPCCX_cells.append(s)\n",
    "                negPCCY_cells.append(numNegPCC)                \n",
    "                print \"++ Generating histogram ++\"\n",
    "                bins = 100\n",
    "                plt.hist(corrMatrix.ravel(), bins, alpha=0.5)                \n",
    "                # Set range for plot appropriately!\n",
    "                plt.xlim(-0.05,0.1)\n",
    "                plt.xlabel(\"PCC\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                plt.savefig(\"cellsHist_\" + str(ts))\n",
    "                plt.close()\n",
    "                print \"++ Computing entropy ++\"\n",
    "                entropyX.append(ts)\n",
    "                entropyY.append(computeEntropy(subSpikeTrains))                \n",
    "                \n",
    "                print \"++ Analyzing correlations (whole columns) ++\"\n",
    "                ### First the LARGE subsample of columns:\n",
    "                subSpikeTrains = subSampleWholeColumn(spikeTrains, colIndicesLarge, tm.getCellsPerColumn(), ts)\n",
    "                (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "                negPCCX_cols.append(s)\n",
    "                negPCCY_cols.append(numNegPCC)                \n",
    "                print \"++ Generating histogram ++\"\n",
    "                bins = 50\n",
    "                plt.hist(corrMatrix.ravel(), bins, alpha=0.5)\n",
    "                plt.xlabel(\"PCC\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                plt.savefig(\"colsHist_\" + str(ts))\n",
    "                plt.close()                 \n",
    "                ### For the heatmap we consider the SMALL subsample in order to understand what's going on\n",
    "                subSpikeTrains = subSampleWholeColumn(spikeTrains, colIndicesSmall, tm.getCellsPerColumn(), ts)\n",
    "                (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)                \n",
    "                print \"++ Generating heatmap ++\"\n",
    "                plt.imshow(corrMatrix, cmap='spectral', interpolation='nearest')\n",
    "                cb = plt.colorbar()\n",
    "                cb.set_label('PCC')\n",
    "                plt.savefig(\"colsHeatMap_\" + str(ts))\n",
    "                plt.close()\n",
    "            ts += 1\n",
    "    allSequences.append(sequence)\n",
    "print \"***All sequences processed!***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot trace of negative PCCs\n",
    "plt.plot(negPCCX_cells, negPCCY_cells)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cells\")\n",
    "plt.close()\n",
    "\n",
    "plt.plot(negPCCX_cols, negPCCY_cols)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cols\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot entropy\n",
    "plt.plot(entropyX, entropyY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.savefig(\"entropyTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(columnUsage)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISI analysis (with Poisson model too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), totalTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isi = computeISI(subSpikeTrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print ISI distribution of TM\n",
    "\n",
    "#bins = np.linspace(np.min(isi), np.max(isi), 50)\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "# plt.xlim(0,4000)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firingRate = 8\n",
    "pSpikeTrain = poissonSpikeGenerator(firingRate,np.shape(subSpikeTrains)[1],np.shape(subSpikeTrains)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isi = computeISI(pSpikeTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print ISI distribution of Poisson model\n",
    "\n",
    "#bins = np.linspace(np.min(isi), np.max(isi), 50)\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "# plt.xlim(0,4000)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiPOI\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 100, tm.numberOfCells(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rasterPlot(subSpikeTrains, \"TM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pSpikeTrain = poissonSpikeGenerator(firingRate,np.shape(subSpikeTrains)[1],np.shape(subSpikeTrains)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rasterPlot(pSpikeTrain, \"Poisson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleAccuracyTest(\"random\", tm, allSequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elad Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample from both TM_SpikeTrains and Poisson_SpikeTrains. 10 cells for 1000 (?) timesteps\n",
    "wordLength = 10\n",
    "firingRate = 8 # This parameter is for the Poisson_SpikeTrains and should be taken experimentally\n",
    "\n",
    "subSpikeTrains = subSample(spikeTrains, wordLength, tm.numberOfCells(), totalTS)\n",
    "pSpikeTrain = poissonSpikeGenerator(firingRate,np.shape(subSpikeTrains)[1],np.shape(subSpikeTrains)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate all 2^10 binary strings\n",
    "binaryStrings = list(itertools.product([0, 1], repeat=wordLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's do the same experiment but with all 2^10 binary strings\n",
    "x = [] #observed\n",
    "y = [] #predicted by random model\n",
    "for i in range(2**wordLength):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i % 100 == 0:\n",
    "        print str(i) + \" words processed\"\n",
    "    binaryWord = np.array(binaryStrings[i], dtype=\"uint32\")\n",
    "    x.append(countInSample(binaryWord, subSpikeTrains))\n",
    "    y.append(countInSample(binaryWord, pSpikeTrain))\n",
    "print \"**All words processed**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.loglog(x, y, 'bo',basex=10)\n",
    "plt.xlabel(\"Observed\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.plot(x,x,'k-')\n",
    "plt.xlim(0,np.max(x))\n",
    "# plt.xlim(0,20)\n",
    "# plt.ylim(0,20)\n",
    "# plt.show()\n",
    "plt.savefig(\"eladPlot\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveTM(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load the TM back from the file do:\n",
    "with open('tm.nta', 'rb') as f:\n",
    "    proto2 = TemporalMemoryProto_capnp.TemporalMemoryProto.read(f, traversal_limit_in_words=2**61)\n",
    "tm = TM.read(proto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlapMatrix = inputAnalysis(allSequences, \"random\", tm.numberOfColumns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show heatmap of overlap matrix\n",
    "plt.imshow(overlapMatrix, cmap='spectral', interpolation='nearest')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Overlap Score')\n",
    "plt.savefig(\"overlapScore_heatmap\")\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "# generate histogram\n",
    "bins = 60\n",
    "(n, bins, patches) = plt.hist(overlapMatrix.ravel(), bins, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist\")\n",
    "\n",
    "plt.xlim(0,0.15)\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist_ZOOM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print colIndicesSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print corrMatrix[0,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.corrcoef(spikeTrains[1824,:], spikeTrains[1826,:])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
